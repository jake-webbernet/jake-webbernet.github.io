---
published: false
---

I'm not an RDS expert and I was recently caught out by not monitoring the IOPS consumed by my RDS Postgres instance.

We recently had a large influx of traffic on one of our websites that tested the durability of our RDS instance. One afternoon at around **17:00** I received an alert email from Amazon Cloudwatch noting that there one of the websites we maintain had an average latency of about 1-2 seconds.

![slow response time]({{site.baseurl}}/_posts/slow_responsse.png)
_Response Time as measured at the load balancer_


I jumped onto the website and confirmed it was at an absolute crawl. 

I immediately checked my Postgres database for any abnormalities. The write latency was 200-300ms. 

![slow write latency rds]({{site.baseurl}}/_posts/write_latency_rds.PNG)
_Write latency of the Postgres RDS database_

Checking the CPU I could see that the CPU baseline had definately changed.

![RDSCPU_2.png]({{site.baseurl}}/_posts/RDSCPU_2.png)

I've experienced this in the past (high write latency and a higher CPU baseline) with a non-production system. A database restart fixed it in that case. 

I contacted the Amazon Support Team with some screenshots of the graphs and a summary of the problem. They advised that the database had exceeded its IOPS quota and was effectively throttled. Admittedly I didn't know a ton about how IOPS was handled in RDS (first mistake).

IOPS is a unit of measurement of input/output operations per second and my database was doing too many. Referencing the CPU chart above, you can actually see the database doing some heavy work just before 17:00.

The Amazon support team is always fantastic, they could have easily referred me to external documentation but instead they curated a proper response related to my actual issue.

Here is a snippet from their response

> Your instance is configured with 100 GB of gp2 storage which gives a baseline performance of 300 IOPS (3 IOPS per GB of provisioned storage) and has the ability to burst up to 3,000 IOPS.  The combined Read [1] and Write IOPS on the instance were consistently above the 300 baseline IOPS of your 100 GB gp2 volume. 

> Because your instance continued to burst above the 300 baseline IOPS for an extended period your instance has ran out of burst IO credits and start being throttled at 300 IOPS. 

> After the burst bucket becomes exhausted your instance storage will be throttled to the 300 IOPS that is allocated, this throttling can lead to increased latency.

The support engineer explained futher that I should either either optimise the workload, or increase my IOPS by allocating more underlying storage.

